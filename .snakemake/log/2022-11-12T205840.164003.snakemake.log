Building DAG of jobs...
Using shell: /opt/tljh/user/bin/bash
Provided cores: 12
Rules claiming more threads will be scaled down.
Job stats:
job          count    min threads    max threads
---------  -------  -------------  -------------
all              1              1              1
get_reads        1              1              1
total            2              1              1

Select jobs to execute...

[Sat Nov 12 20:58:40 2022]
rule get_reads:
    input: /home/jupyter-contacttonyliu/biof501-pipeline
    output: /home/jupyter-contacttonyliu/biof501-pipeline/workspace/run_01/reads
    jobid: 1
    reason: Missing output files: /home/jupyter-contacttonyliu/biof501-pipeline/workspace/run_01/reads
    resources: tmpdir=/tmp

ImproperOutputException in rule get_reads  in line 21 of /home/jupyter-contacttonyliu/biof501-pipeline/main.smk:
Outputs of incorrect type (directories when expecting files or vice versa). Output directories must be flagged with directory(). for rule get_reads:
    output: /home/jupyter-contacttonyliu/biof501-pipeline/workspace/run_01/reads
    affected files:
        /home/jupyter-contacttonyliu/biof501-pipeline/workspace/run_01/reads
Removing output files of failed job get_reads since they might be corrupted:
/home/jupyter-contacttonyliu/biof501-pipeline/workspace/run_01/reads
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2022-11-12T205840.164003.snakemake.log
